# ğŸ“˜ Day 4 Notes â€” Decision Trees & Random Forest (Ensemble Learning)

## ğŸ¯ Objective

Understand:

- How Decision Trees work internally  
- How splitting criteria (Gini, Entropy) function  
- What overfitting in trees looks like  
- How Random Forest improves performance using ensemble learning  
- How to tune tree-based models  
- When to use Trees vs Logistic Regression  

---

## ğŸŸ¦ 1. What is a Decision Tree?

A **Decision Tree** is a supervised ML algorithm used for:

- Classification  
- Regression  

It works like a **flowchart**:

- Each internal node = a question on a feature  
- Each branch = outcome of the question  
- Each leaf node = final prediction  

It mimics **human decision-making**.

---

## ğŸŸ¦ 2. Example of a Decision Tree (Concept)

```text
Is Age > 45?
 â”‚
 â”œâ”€ Yes â†’ Is BP High?
 â”‚         â”œâ”€ Yes â†’ Disease
 â”‚         â””â”€ No  â†’ No Disease
 â”‚
 â””â”€ No â†’ No Disease
```
Trees split data based on the **best question** to ask at each step.

---

## ğŸŸ¦ 3. Key Terminology

- **Root Node** â†’ First split  
- **Decision Node** â†’ Intermediate split  
- **Leaf Node** â†’ Final prediction  
- **Depth** â†’ Length of the longest path  
- **Pruning** â†’ Cutting unnecessary branches  
- **Feature Importance** â†’ How important a feature is in decisions  

---

## ğŸŸ¦ 4. How Does a Decision Tree Decide the Best Split?

Uses **impurity measures**:

### âœ… Gini Impurity
- Measures misclassification likelihood  
- Range:  
  - `0` â†’ Pure (only one class)  
  - `> 0` â†’ Mixed classes  
- **Lower Gini = better split**

### âœ… Entropy (Information Gain)
- Measures **disorder**  
- High entropy â†’ Mixed classes  
- Low entropy â†’ Pure classes  
- **Information Gain = Entropy before split â€“ Entropy after split**  
- Split with **maximum information gain** is selected.  

---

## ğŸŸ¦ 5. Stopping Criteria in Decision Trees

Tree stops growing when:

- `max_depth` is reached  
- Node becomes pure  
- `min_samples_leaf` is reached  
- No further information gain is possible  

Without stopping â†’ **overfitting happens**.

---

## ğŸŸ¦ 6. Overfitting in Decision Trees

Decision Trees **memorize training data easily**.

### Symptoms:
- Training accuracy â‰ˆ 100%  
- Test accuracy is low  

### Solutions:
- Limit `max_depth`  
- Set `min_samples_split`  
- Set `min_samples_leaf`  
- Use pruning  
- Use Random Forest  

---

## ğŸŸ¦ 7. What is Random Forest?

A **Random Forest** is an ensemble of many Decision Trees.

Instead of relying on a single tree:

- Builds **multiple trees**  

Each tree sees:

- A **random subset of data**  
- A **random subset of features**  

### Final Prediction:
- **Majority voting** (Classification)  
- **Average prediction** (Regression)  

---

## ğŸŸ¦ 8. Why Random Forest is Better Than a Single Tree?

| Decision Tree     | Random Forest          |
|-------------------|------------------------|
| High variance     | Low variance           |
| Overfits easily   | Resistant to overfitting |
| Unstable          | Very stable            |
| Sensitive to noise| Robust to noise        |

âœ… **Random Forest = High accuracy + Stability**

---

## ğŸŸ¦ 9. Bagging (Bootstrap Aggregation)

Random Forest uses **Bagging**:

- Sample data **with replacement**  
- Train a tree on each random sample  
- Combine predictions  

âœ… This **reduces variance dramatically**.

---

## ğŸŸ¦ 10. Feature Importance in Random Forest

Random Forest can automatically tell:

- Which feature **contributes most**  
- Which features can be **ignored**  

### Useful for:
- Medical diagnosis  
- Finance risk modeling  
- Feature selection before deep learning  

---

## ğŸŸ¦ 11. Hyperparameters to Tune

### âœ… Decision Tree:
- `max_depth`  
- `min_samples_split`  
- `min_samples_leaf`  
- `criterion` (`gini` / `entropy`)  

### âœ… Random Forest:
- `n_estimators`  
- `max_depth`  
- `min_samples_split`  
- `max_features`  

---

## ğŸŸ¦ 12. Classification vs Regression Trees

| Classification Tree      | Regression Tree      |
|--------------------------|----------------------|
| Uses Gini / Entropy      | Uses MSE             |
| Outputs class            | Outputs number       |
| Used for spam, disease   | Used for house price |

---

## ğŸŸ¦ 13. When to Use Decision Trees?

Use Decision Trees when:

- Data is **non-linear**  
- You want **model interpretability**  
- You want **rule-based predictions**  

---

## ğŸŸ¦ 14. When to Use Random Forest?

Use Random Forest when:

- Single tree is **overfitting**  
- **High accuracy** is required  
- Data is **noisy**  
- Interpretability is **less important** than performance  

---

## ğŸŸ¦ 15. Advantages & Disadvantages

### âœ… Decision Tree â€“ Pros
- Easy to understand  
- No feature scaling needed  
- Works on both classification & regression  

### âŒ Decision Tree â€“ Cons
- Overfits easily  
- Unstable with small changes in data  

### âœ… Random Forest â€“ Pros
- High accuracy  
- Less overfitting  
- Handles missing values well  

### âŒ Random Forest â€“ Cons
- Slower than a single tree  
- Harder to interpret  
- More memory usage  

---

## ğŸŸ¦ 16. Interview Takeaways

You can now confidently answer:

- How does a Decision Tree work?  
- What is Gini impurity?  
- Difference between Gini and Entropy?  
- Why do Decision Trees overfit?  
- What is Random Forest?  
- What is bagging?  
- Why is Random Forest better than a single tree?  
- Difference between bagging and boosting?  
- What is feature importance?  

---

## ğŸŸ¦ 17. Final Summary

âœ” Learned Decision Tree fundamentals  
âœ” Understood Gini vs Entropy  
âœ” Learned stopping criteria and overfitting control  
âœ” Understood Random Forest as an ensemble  
âœ” Learned Bagging technique  
âœ” Understood feature importance  
âœ” Learned when to use Trees vs Logistic Regression  
âœ” Gained interview-ready understanding of tree-based models  

---

## ğŸ“š Day 4 Resources

### ğŸ¥ Video Resources

- Decision Trees Intuition â€” StatQuest  
- Random Forest Explained â€” StatQuest  
- Decision Tree with Scikit-learn  

### ğŸ“– Reading Materials

Decision Tree â€” GeeksforGeeks
https://www.geeksforgeeks.org/decision-tree-implementation-python/

Random Forest â€” Machine Learning Mastery
https://machinelearningmastery.com/random-forest-ensemble-machine-learning/

Gini vs Entropy Explained
https://www.geeksforgeeks.org/gini-index-vs-entropy/

### ğŸ§‘â€ğŸ’» Practice Materials

Scikit-learn Decision Tree Docs
https://scikit-learn.org/stable/modules/tree.html

Scikit-learn Random Forest Docs
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

Kaggle Tree-Based Practice
https://www.kaggle.com/learn/decision-trees